"""
å¹¶è¡Œå¤„ç†å™¨æ¨¡å—

æä¾›æ‰¹é‡å›¾ç‰‡çš„å¹¶è¡Œå¤„ç†èƒ½åŠ›ã€‚
"""

from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
import time
import threading

from ..config import get_config
from ..utils import (
    get_image_size,
    save_annotation,
    to_xanylabeling_format,
    get_logger,
    TaskProgress
)
from .detector import ObjectDetector
from .sign_classifier import SignClassifier


class ParallelProcessor:
    """
    å¹¶è¡Œæ‰¹é‡å¤„ç†å™¨
    
    æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†å›¾ç‰‡ï¼Œæä¾›è¿›åº¦è·Ÿè¸ªå’Œé”™è¯¯å¤„ç†ã€‚
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        workers: int = 5,
        use_rag: bool = False
    ):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            api_key: API Key
            workers: å¹¶è¡Œçº¿ç¨‹æ•°
            use_rag: æ˜¯å¦å¯ç”¨ RAG ç»†ç²’åº¦åˆ†ç±»
        """
        self.config = get_config()
        self.logger = get_logger()
        
        self.api_key = api_key or self.config.api_key
        self.workers = workers
        self.use_rag = use_rag

        # ç»Ÿè®¡æ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        self._stats_lock = threading.Lock()
        self.stats = {
            "pedestrian": 0,
            "vehicle": 0,
            "traffic_sign": 0,
            "construction": 0
        }
    
    def process_batch(
        self,
        image_paths: List[str],
        output_dir: Path,
        on_complete: Optional[Callable] = None,
        resume: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å›¾ç‰‡
        
        Args:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            on_complete: å•å¼ å®Œæˆå›è°ƒ (path, detections, error) -> None
            resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆè·³è¿‡å·²å¤„ç†çš„å›¾ç‰‡ï¼‰
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–­ç‚¹ç»­ä¼ ï¼šè¿‡æ»¤å·²å¤„ç†çš„å›¾ç‰‡
        if resume:
            original_count = len(image_paths)
            image_paths = self._filter_processed(image_paths, output_dir)
            skipped = original_count - len(image_paths)
            
            if skipped > 0:
                self.logger.info(f"ğŸ“Œ æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡ {skipped} å¼ å·²å¤„ç†å›¾ç‰‡")
            
            if not image_paths:
                self.logger.info("âœ… æ‰€æœ‰å›¾ç‰‡å·²å¤„ç†å®Œæˆï¼Œæ— éœ€é‡æ–°è¿è¡Œ")
                return {
                    "success": 0, 
                    "failed": 0, 
                    "skipped": skipped,
                    "total_objects": 0,
                    "stats": self.stats
                }
        
        progress = TaskProgress(len(image_paths), "Parallel Detection")
        progress.start()
        
        start_time = time.time()
        results = {"success": 0, "failed": 0, "total_objects": 0}
        
        with ThreadPoolExecutor(max_workers=self.workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {
                executor.submit(self._process_single, path): path
                for path in image_paths
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                image_path = futures[future]
                image_name = Path(image_path).name
                
                try:
                    detections, error = future.result()
                    
                    if error:
                        progress.update(image_name, success=False, message=error)
                        results["failed"] += 1
                    else:
                        # æ›´æ–°ç»Ÿè®¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                        with self._stats_lock:
                            for det in detections:
                                category = det.get("category", "unknown")
                                self.stats[category] = self.stats.get(category, 0) + 1
                                results["total_objects"] += 1
                        
                        # ä¿å­˜æ ‡æ³¨
                        self._save_result(detections, image_path, output_dir)
                        
                        progress.update(
                            image_name, 
                            success=True, 
                            message=f"{len(detections)} objects"
                        )
                        results["success"] += 1
                    
                    # å›è°ƒ
                    if on_complete:
                        on_complete(image_path, detections, error)
                        
                except Exception as e:
                    progress.update(image_name, success=False, message=str(e))
                    results["failed"] += 1
        
        elapsed = time.time() - start_time
        results["elapsed_seconds"] = elapsed
        results["per_image_seconds"] = elapsed / len(image_paths) if image_paths else 0
        results["stats"] = self.stats
        
        progress.finish(extra_stats=self.stats)
        
        return results
    
    def _process_single(self, image_path: str) -> tuple:
        """
        å¤„ç†å•å¼ å›¾ç‰‡ï¼ˆåœ¨å·¥ä½œçº¿ç¨‹ä¸­æ‰§è¡Œï¼‰

        Returns:
            (detections, error)
        """
        image_name = Path(image_path).name

        try:
            # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ£€æµ‹å™¨
            detector = ObjectDetector(api_key=self.api_key)
            detections = detector.detect(image_path)

            # RAG ç»†ç²’åº¦åˆ†ç±»
            if self.use_rag and detections:
                classifier = SignClassifier(api_key=self.api_key)

                for det in detections:
                    if det["category"] == "traffic_sign" and det["label"] in ["traffic_sign", "sign"]:
                        det["label"] = classifier.classify(image_path, det["bbox"])

            return detections, None

        except FileNotFoundError as e:
            self.logger.error(f"[{image_name}] File not found: {e}")
            return [], f"FileNotFoundError: {e}"

        except ValueError as e:
            self.logger.error(f"[{image_name}] Invalid input: {e}")
            return [], f"ValueError: {e}"

        except Exception as e:
            self.logger.error(f"[{image_name}] Unexpected error: {type(e).__name__}: {e}")
            return [], f"{type(e).__name__}: {e}"
    
    def _save_result(
        self, 
        detections: List[Dict], 
        image_path: str, 
        output_dir: Path
    ):
        """ä¿å­˜æ ‡æ³¨ç»“æœ"""
        width, height = get_image_size(image_path)
        annotation = to_xanylabeling_format(detections, image_path, width, height)
        
        output_path = output_dir / f"{Path(image_path).stem}.json"
        save_annotation(annotation, output_path)
    
    def _filter_processed(
        self,
        image_paths: List[str],
        output_dir: Path
    ) -> List[str]:
        """
        è¿‡æ»¤å·²å¤„ç†çš„å›¾ç‰‡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        
        æ£€æŸ¥è¾“å‡ºç›®å½•ä¸­æ˜¯å¦å·²å­˜åœ¨å¯¹åº”çš„ JSON æ–‡ä»¶ï¼Œ
        å¦‚æœå­˜åœ¨åˆ™è®¤ä¸ºè¯¥å›¾ç‰‡å·²å¤„ç†ï¼Œè·³è¿‡ã€‚
        
        Args:
            image_paths: åŸå§‹å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æœªå¤„ç†çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        """
        unprocessed = []
        
        for path in image_paths:
            json_name = f"{Path(path).stem}.json"
            json_path = output_dir / json_name
            
            if not json_path.exists():
                unprocessed.append(path)
        
        return unprocessed


def process_images_parallel(
    image_paths: List[str],
    output_dir: str,
    workers: int = 5,
    use_rag: bool = False,
    api_key: Optional[str] = None
) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šå¹¶è¡Œå¤„ç†å›¾ç‰‡æ‰¹æ¬¡
    
    Args:
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        workers: å¹¶è¡Œçº¿ç¨‹æ•°
        use_rag: æ˜¯å¦å¯ç”¨ RAG
        api_key: API Key
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    processor = ParallelProcessor(
        api_key=api_key,
        workers=workers,
        use_rag=use_rag
    )
    return processor.process_batch(image_paths, Path(output_dir))
