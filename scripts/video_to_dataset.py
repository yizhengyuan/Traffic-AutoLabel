#!/usr/bin/env python3
"""
ğŸš€ è§†é¢‘åˆ°æ•°æ®é›†ä¸€é”®æµæ°´çº¿

å®Œæ•´æµç¨‹ï¼š
1. ä»è§†é¢‘æŠ½å¸§ (3 FPS)
2. è‡ªåŠ¨æ ‡æ³¨ (GLM-4.6V + RAG)
3. ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡
4. æ‰“åŒ…æˆ Dataset æ–‡ä»¶å¤¹

ç”¨æ³•:
    python3 scripts/video_to_dataset.py --video D3
    python3 scripts/video_to_dataset.py --video D4 --workers 20 --rag
"""

import os
import sys
import json
import argparse
import subprocess
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ  scripts ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from auto_labeling_parallel import (
    process_single_image,
    to_xanylabeling_format,
    get_image_size
)


# ============================================================================
# é…ç½®
# ============================================================================

VIDEO_DIR = Path("traffic_sign_data/videos/raw_clips")
TEMP_FRAMES_DIR = Path("temp_frames")
OUTPUT_BASE = Path("output")

COLORS = {
    'pedestrian': (255, 0, 0),
    'vehicle': (0, 255, 0),
    'traffic_sign': (0, 100, 255),
    'construction': (255, 165, 0),
}


# ============================================================================
# Step 1: æŠ½å¸§
# ============================================================================

def extract_frames(video_name: str, fps: int = 3) -> tuple:
    """ä»è§†é¢‘æŠ½å¸§"""
    video_path = VIDEO_DIR / f"{video_name}.mp4"
    
    if not video_path.exists():
        print(f"âŒ è§†é¢‘ä¸å­˜åœ¨: {video_path}")
        return None, 0
    
    # åˆ›å»ºä¸´æ—¶å¸§ç›®å½•
    frames_dir = TEMP_FRAMES_DIR / video_name
    frames_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç©ºæ—§å¸§
    for old_frame in frames_dir.glob("*.jpg"):
        old_frame.unlink()
    
    print(f"\nğŸ“¹ Step 1: æŠ½å¸§ ({fps} FPS)")
    print(f"   è§†é¢‘: {video_path}")
    
    # ä½¿ç”¨ ffmpeg æŠ½å¸§
    output_pattern = str(frames_dir / f"{video_name}_%06d.jpg")
    cmd = [
        "ffmpeg", "-i", str(video_path),
        "-vf", f"fps={fps}",
        "-q:v", "2",  # é«˜è´¨é‡
        output_pattern,
        "-y"  # è¦†ç›–
    ]
    
    try:
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        frame_count = len(list(frames_dir.glob("*.jpg")))
        print(f"   âœ… æŠ½å– {frame_count} å¸§")
        
        return frames_dir, frame_count
        
    except subprocess.TimeoutExpired:
        print("   âŒ ffmpeg è¶…æ—¶")
        return None, 0
    except Exception as e:
        print(f"   âŒ ffmpeg é”™è¯¯: {e}")
        return None, 0


# ============================================================================
# Step 2: è‡ªåŠ¨æ ‡æ³¨
# ============================================================================

def run_labeling(frames_dir: Path, video_name: str, workers: int, use_rag: bool) -> Path:
    """è¿è¡Œè‡ªåŠ¨æ ‡æ³¨"""
    print(f"\nğŸ·ï¸ Step 2: è‡ªåŠ¨æ ‡æ³¨")
    print(f"   Workers: {workers} | RAG: {'âœ…' if use_rag else 'âŒ'}")
    
    api_key = os.getenv("ZAI_API_KEY")
    if not api_key:
        print("   âŒ è¯·è®¾ç½® ZAI_API_KEY")
        return None
    
    # è·å–å¸§åˆ—è¡¨
    image_files = sorted(frames_dir.glob("*.jpg"))
    if not image_files:
        print("   âŒ æ²¡æœ‰æ‰¾åˆ°å¸§")
        return None
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    rag_suffix = "_rag" if use_rag else ""
    output_dir = OUTPUT_BASE / f"{video_name.lower()}_annotations{rag_suffix}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å‡†å¤‡ä»»åŠ¡
    task_args = [(str(img), api_key, 3, use_rag) for img in image_files]
    
    start_time = time.time()
    stats = {"pedestrian": 0, "vehicle": 0, "traffic_sign": 0, "construction": 0}
    success = 0
    errors = 0
    
    # å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {executor.submit(process_single_image, arg): arg[0] for arg in task_args}
        
        for i, future in enumerate(as_completed(futures)):
            image_path = futures[future]
            image_name = Path(image_path).name
            
            try:
                _, detections, error = future.result()
                
                if error:
                    print(f"  âš ï¸ [{i+1}/{len(image_files)}] {error}")
                    errors += 1
                else:
                    for det in detections:
                        cat = det.get("category", "unknown")
                        stats[cat] = stats.get(cat, 0) + 1
                    
                    # ä¿å­˜
                    annotation = to_xanylabeling_format(detections, image_path)
                    out_path = output_dir / f"{Path(image_path).stem}.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(annotation, f, ensure_ascii=False, indent=2)
                    
                    emoji = "âœ…" if detections else "âšª"
                    print(f"  {emoji} [{i+1}/{len(image_files)}] {len(detections)} objects")
                    success += 1
                    
            except Exception as e:
                print(f"  âŒ [{i+1}/{len(image_files)}] {e}")
                errors += 1
    
    elapsed = time.time() - start_time
    print(f"\n   ğŸ“Š ç»Ÿè®¡: {stats}")
    print(f"   â±ï¸ è€—æ—¶: {elapsed:.1f}s ({elapsed/len(image_files):.2f}s/å¸§)")
    print(f"   âœ… æˆåŠŸ: {success} | âŒ é”™è¯¯: {errors}")
    
    return output_dir


# ============================================================================
# Step 3: å¯è§†åŒ–
# ============================================================================

def generate_visualizations(frames_dir: Path, annotations_dir: Path, video_name: str) -> Path:
    """ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡"""
    from PIL import Image, ImageDraw
    
    print(f"\nğŸ¨ Step 3: ç”Ÿæˆå¯è§†åŒ–")
    
    vis_dir = OUTPUT_BASE / f"{video_name.lower()}_visualized"
    vis_dir.mkdir(parents=True, exist_ok=True)
    
    count = 0
    for json_path in sorted(annotations_dir.glob("*.json")):
        frame_name = json_path.stem + ".jpg"
        frame_path = frames_dir / frame_name
        
        if not frame_path.exists():
            continue
        
        img = Image.open(frame_path)
        draw = ImageDraw.Draw(img)
        
        with open(json_path) as f:
            data = json.load(f)
        
        for shape in data.get("shapes", []):
            pts = shape["points"]
            cat = shape.get("flags", {}).get("category", "unknown")
            label = shape["label"]
            
            color = COLORS.get(cat, (128, 128, 128))
            draw.rectangle([pts[0][0], pts[0][1], pts[1][0], pts[1][1]], 
                          outline=color, width=3)
            
            # æ ‡ç­¾
            short_label = label[:20] + "..." if len(label) > 20 else label
            draw.text((pts[0][0], pts[0][1] - 15), short_label, fill=color)
        
        out_path = vis_dir / f"{json_path.stem}_vis.jpg"
        img.save(out_path)
        count += 1
        
        if count % 50 == 0:
            print(f"   å·²å¤„ç† {count} å¼ ...")
    
    print(f"   âœ… ç”Ÿæˆ {count} å¼ å¯è§†åŒ–å›¾ç‰‡")
    return vis_dir


# ============================================================================
# Step 4: æ‰“åŒ… Dataset
# ============================================================================

def create_dataset(video_name: str, frames_dir: Path, annotations_dir: Path, vis_dir: Path) -> Path:
    """åˆ›å»º Dataset æ–‡ä»¶å¤¹"""
    import shutil
    
    print(f"\nğŸ“¦ Step 4: åˆ›å»º Dataset")
    
    dataset_dir = Path(f"{video_name}_dataset")
    
    # åˆ›å»ºç›®å½•ç»“æ„
    (dataset_dir / "video").mkdir(parents=True, exist_ok=True)
    (dataset_dir / "frames").mkdir(parents=True, exist_ok=True)
    (dataset_dir / "annotations").mkdir(parents=True, exist_ok=True)
    (dataset_dir / "visualized").mkdir(parents=True, exist_ok=True)
    
    # å¤åˆ¶è§†é¢‘
    video_src = VIDEO_DIR / f"{video_name}.mp4"
    if video_src.exists():
        shutil.copy(video_src, dataset_dir / "video" / f"{video_name}.mp4")
        print(f"   âœ… å¤åˆ¶è§†é¢‘")
    
    # å¤åˆ¶å¸§
    for frame in frames_dir.glob("*.jpg"):
        shutil.copy(frame, dataset_dir / "frames" / frame.name)
    print(f"   âœ… å¤åˆ¶ {len(list(frames_dir.glob('*.jpg')))} å¸§")
    
    # å¤åˆ¶æ ‡æ³¨
    for ann in annotations_dir.glob("*.json"):
        shutil.copy(ann, dataset_dir / "annotations" / ann.name)
    print(f"   âœ… å¤åˆ¶ {len(list(annotations_dir.glob('*.json')))} æ ‡æ³¨")
    
    # å¤åˆ¶å¯è§†åŒ–
    if vis_dir and vis_dir.exists():
        for vis in vis_dir.glob("*.jpg"):
            shutil.copy(vis, dataset_dir / "visualized" / vis.name)
        print(f"   âœ… å¤åˆ¶ {len(list(vis_dir.glob('*.jpg')))} å¯è§†åŒ–")
    
    # ç”Ÿæˆå‹ç¼©åŒ…
    print(f"   ğŸ“¦ åˆ›å»ºå‹ç¼©åŒ…...")
    shutil.make_archive(str(dataset_dir), 'zip', '.', str(dataset_dir))
    zip_size = Path(f"{dataset_dir}.zip").stat().st_size / (1024 * 1024)
    print(f"   âœ… {dataset_dir}.zip ({zip_size:.1f} MB)")
    
    return dataset_dir


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="è§†é¢‘åˆ°æ•°æ®é›†ä¸€é”®æµæ°´çº¿")
    parser.add_argument("--video", type=str, required=True, help="è§†é¢‘åç§° (å¦‚ D3, D4)")
    parser.add_argument("--fps", type=int, default=3, help="æŠ½å¸§ç‡ (é»˜è®¤ 3)")
    parser.add_argument("--workers", type=int, default=20, help="å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤ 20)")
    parser.add_argument("--rag", action="store_true", help="å¯ç”¨ RAG ç»†ç²’åº¦åˆ†ç±»")
    parser.add_argument("--skip-extract", action="store_true", help="è·³è¿‡æŠ½å¸§æ­¥éª¤")
    parser.add_argument("--skip-visualize", action="store_true", help="è·³è¿‡å¯è§†åŒ–æ­¥éª¤")
    args = parser.parse_args()
    
    video_name = args.video
    
    print("=" * 70)
    print(f"ğŸš€ è§†é¢‘åˆ°æ•°æ®é›†æµæ°´çº¿ - {video_name}")
    print(f"   FPS: {args.fps} | Workers: {args.workers} | RAG: {args.rag}")
    print("=" * 70)
    
    start_time = time.time()
    
    # Step 1: æŠ½å¸§
    if args.skip_extract:
        frames_dir = TEMP_FRAMES_DIR / video_name
        print(f"\nâ­ï¸ è·³è¿‡æŠ½å¸§ï¼Œä½¿ç”¨: {frames_dir}")
    else:
        frames_dir, frame_count = extract_frames(video_name, args.fps)
        if not frames_dir:
            return
    
    # Step 2: æ ‡æ³¨
    annotations_dir = run_labeling(frames_dir, video_name, args.workers, args.rag)
    if not annotations_dir:
        return
    
    # Step 3: å¯è§†åŒ–
    if args.skip_visualize:
        vis_dir = None
        print(f"\nâ­ï¸ è·³è¿‡å¯è§†åŒ–")
    else:
        vis_dir = generate_visualizations(frames_dir, annotations_dir, video_name)
    
    # Step 4: æ‰“åŒ…
    dataset_dir = create_dataset(video_name, frames_dir, annotations_dir, vis_dir)
    
    # å®Œæˆ
    total_time = time.time() - start_time
    print("\n" + "=" * 70)
    print(f"ğŸ‰ å®Œæˆï¼æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“ Dataset: {dataset_dir}/")
    print(f"ğŸ“¦ å‹ç¼©åŒ…: {dataset_dir}.zip")
    print("=" * 70)


if __name__ == "__main__":
    main()
