#!/usr/bin/env python3
"""
ğŸš€ è§†é¢‘åˆ°æ•°æ®é›†ä¸€é”®æµæ°´çº¿

å®Œæ•´æµç¨‹ï¼š
1. ä»è§†é¢‘æŠ½å¸§ (3 FPS)
2. è‡ªåŠ¨æ ‡æ³¨ (GLM-4.6V + RAG)
3. ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡
4. æ‰“åŒ…æˆ Dataset æ–‡ä»¶å¤¹

ç”¨æ³•:
    python3 scripts/video_to_dataset.py --video D3
    python3 scripts/video_to_dataset.py --video D4 --workers 20 --rag
"""

import os
import sys
import json
import argparse
import subprocess
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ  scripts ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from auto_labeling_parallel import (
    process_single_image,
    to_xanylabeling_format,
    get_image_size
)


# ============================================================================
# é…ç½®
# ============================================================================

VIDEO_DIR = Path("traffic_sign_data/videos/raw_videos")
TEMP_FRAMES_DIR = Path("temp_frames")
OUTPUT_BASE = Path("output")

COLORS = {
    'pedestrian': (255, 0, 0),
    'vehicle': (0, 255, 0),
    'traffic_sign': (0, 100, 255),
    'construction': (255, 165, 0),
}


# ============================================================================
# Step 1: æŠ½å¸§
# ============================================================================

def extract_frames(video_name: str, fps: int = 3) -> tuple:
    """ä»è§†é¢‘æŠ½å¸§"""
    video_path = VIDEO_DIR / f"{video_name}.mp4"
    
    if not video_path.exists():
        print(f"âŒ è§†é¢‘ä¸å­˜åœ¨: {video_path}")
        return None, 0
    
    # åˆ›å»ºä¸´æ—¶å¸§ç›®å½•
    frames_dir = TEMP_FRAMES_DIR / video_name
    frames_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç©ºæ—§å¸§
    for old_frame in frames_dir.glob("*.jpg"):
        old_frame.unlink()
    
    print(f"\nğŸ“¹ Step 1: æŠ½å¸§ ({fps} FPS)")
    print(f"   è§†é¢‘: {video_path}")
    
    # ä½¿ç”¨ ffmpeg æŠ½å¸§
    output_pattern = str(frames_dir / f"{video_name}_%06d.jpg")
    cmd = [
        "ffmpeg", "-i", str(video_path),
        "-vf", f"fps={fps}",
        "-q:v", "2",  # é«˜è´¨é‡
        output_pattern,
        "-y"  # è¦†ç›–
    ]
    
    try:
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        frame_count = len(list(frames_dir.glob("*.jpg")))
        print(f"   âœ… æŠ½å– {frame_count} å¸§")
        
        return frames_dir, frame_count
        
    except subprocess.TimeoutExpired:
        print("   âŒ ffmpeg è¶…æ—¶")
        return None, 0
    except Exception as e:
        print(f"   âŒ ffmpeg é”™è¯¯: {e}")
        return None, 0


# ============================================================================
# Step 2: è‡ªåŠ¨æ ‡æ³¨
# ============================================================================

def run_labeling(frames_dir: Path, video_name: str, workers: int, use_rag: bool) -> Path:
    """è¿è¡Œè‡ªåŠ¨æ ‡æ³¨"""
    print(f"\nğŸ·ï¸ Step 2: è‡ªåŠ¨æ ‡æ³¨")
    print(f"   Workers: {workers} | RAG: {'âœ…' if use_rag else 'âŒ'}")
    
    api_key = os.getenv("ZAI_API_KEY")
    if not api_key:
        print("   âŒ è¯·è®¾ç½® ZAI_API_KEY")
        return None
    
    # è·å–å¸§åˆ—è¡¨
    image_files = sorted(frames_dir.glob("*.jpg"))
    if not image_files:
        print("   âŒ æ²¡æœ‰æ‰¾åˆ°å¸§")
        return None
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    rag_suffix = "_rag" if use_rag else ""
    output_dir = OUTPUT_BASE / f"{video_name.lower()}_annotations{rag_suffix}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å‡†å¤‡ä»»åŠ¡
    task_args = [(str(img), api_key, 3, use_rag) for img in image_files]
    
    start_time = time.time()
    stats = {"pedestrian": 0, "vehicle": 0, "traffic_sign": 0, "construction": 0}
    success = 0
    errors = 0
    
    # å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {executor.submit(process_single_image, arg): arg[0] for arg in task_args}
        
        for i, future in enumerate(as_completed(futures)):
            image_path = futures[future]
            image_name = Path(image_path).name
            
            try:
                _, detections, error = future.result()
                
                if error:
                    print(f"  âš ï¸ [{i+1}/{len(image_files)}] {error}")
                    errors += 1
                else:
                    for det in detections:
                        cat = det.get("category", "unknown")
                        stats[cat] = stats.get(cat, 0) + 1
                    
                    # ä¿å­˜
                    annotation = to_xanylabeling_format(detections, image_path)
                    out_path = output_dir / f"{Path(image_path).stem}.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(annotation, f, ensure_ascii=False, indent=2)
                    
                    emoji = "âœ…" if detections else "âšª"
                    print(f"  {emoji} [{i+1}/{len(image_files)}] {len(detections)} objects")
                    success += 1
                    
            except Exception as e:
                print(f"  âŒ [{i+1}/{len(image_files)}] {e}")
                errors += 1
    
    elapsed = time.time() - start_time
    print(f"\n   ğŸ“Š ç»Ÿè®¡: {stats}")
    print(f"   â±ï¸ è€—æ—¶: {elapsed:.1f}s ({elapsed/len(image_files):.2f}s/å¸§)")
    print(f"   âœ… æˆåŠŸ: {success} | âŒ é”™è¯¯: {errors}")
    
    return output_dir


# ============================================================================
# Step 3: å¯è§†åŒ–
# ============================================================================

def generate_visualizations(frames_dir: Path, annotations_dir: Path, video_name: str) -> Path:
    """ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡"""
    from PIL import Image, ImageDraw
    
    print(f"\nğŸ¨ Step 3: ç”Ÿæˆå¯è§†åŒ–")
    
    vis_dir = OUTPUT_BASE / f"{video_name.lower()}_visualized"
    vis_dir.mkdir(parents=True, exist_ok=True)
    
    count = 0
    for json_path in sorted(annotations_dir.glob("*.json")):
        frame_name = json_path.stem + ".jpg"
        frame_path = frames_dir / frame_name
        
        if not frame_path.exists():
            continue
        
        img = Image.open(frame_path)
        draw = ImageDraw.Draw(img)
        
        with open(json_path) as f:
            data = json.load(f)
        
        for shape in data.get("shapes", []):
            pts = shape["points"]
            cat = shape.get("flags", {}).get("category", "unknown")
            label = shape["label"]
            
            color = COLORS.get(cat, (128, 128, 128))
            draw.rectangle([pts[0][0], pts[0][1], pts[1][0], pts[1][1]], 
                          outline=color, width=3)
            
            # æ ‡ç­¾
            short_label = label[:20] + "..." if len(label) > 20 else label
            draw.text((pts[0][0], pts[0][1] - 15), short_label, fill=color)
        
        out_path = vis_dir / f"{json_path.stem}_vis.jpg"
        img.save(out_path)
        count += 1
        
        if count % 50 == 0:
            print(f"   å·²å¤„ç† {count} å¼ ...")
    
    print(f"   âœ… ç”Ÿæˆ {count} å¼ å¯è§†åŒ–å›¾ç‰‡")
    return vis_dir


# ============================================================================
# Step 4: æ‰“åŒ… Dataset
# ============================================================================

def generate_summary(annotations_dir: Path, video_name: str, frame_count: int, fps: int) -> dict:
    """åˆ†ææ ‡æ³¨æ•°æ®å¹¶ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    from collections import defaultdict
    from datetime import datetime
    
    stats = {
        "total_frames": frame_count,
        "annotated_frames": 0,
        "total_objects": 0,
        "categories": defaultdict(int),
        "subcategories": defaultdict(int),
        "scene_events": defaultdict(list),  # åœºæ™¯äº‹ä»¶ï¼ˆåˆ¹è½¦ã€è½¬å‘ç­‰ï¼‰
        "frame_details": []
    }
    
    # éå†æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
    for json_path in sorted(annotations_dir.glob("*.json")):
        frame_name = json_path.stem
        
        with open(json_path, encoding="utf-8") as f:
            data = json.load(f)
        
        shapes = data.get("shapes", [])
        if shapes:
            stats["annotated_frames"] += 1
        
        frame_info = {
            "frame": frame_name,
            "objects": len(shapes),
            "categories": defaultdict(int),
            "labels": []
        }
        
        for shape in shapes:
            stats["total_objects"] += 1
            
            # ä¸»ç±»åˆ«
            category = shape.get("flags", {}).get("category", "unknown")
            stats["categories"][category] += 1
            frame_info["categories"][category] += 1
            
            # æ ‡ç­¾ï¼ˆç»†åˆ†ç±»åˆ«ï¼‰
            label = shape.get("label", "")
            frame_info["labels"].append(label)
            
            # ç»Ÿè®¡å­ç±»åˆ«
            if label:
                stats["subcategories"][label] += 1
            
            # æ£€æµ‹åœºæ™¯äº‹ä»¶
            label_lower = label.lower()
            if any(kw in label_lower for kw in ["brake", "braking", "åˆ¹è½¦"]):
                stats["scene_events"]["braking"].append(frame_name)
            if any(kw in label_lower for kw in ["turn_left", "left_turn", "å·¦è½¬"]):
                stats["scene_events"]["turn_left"].append(frame_name)
            if any(kw in label_lower for kw in ["turn_right", "right_turn", "å³è½¬"]):
                stats["scene_events"]["turn_right"].append(frame_name)
            if any(kw in label_lower for kw in ["hazard", "emergency", "åŒé—ª"]):
                stats["scene_events"]["hazard_lights"].append(frame_name)
            if any(kw in label_lower for kw in ["crossing", "è¿‡é©¬è·¯"]):
                stats["scene_events"]["pedestrian_crossing"].append(frame_name)
        
        stats["frame_details"].append(frame_info)
    
    return stats


def create_summary_markdown(stats: dict, video_name: str, fps: int, use_rag: bool) -> str:
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æ€»ç»“æ–‡æ¡£"""
    from datetime import datetime
    
    lines = []
    lines.append(f"# ğŸ“Š æ•°æ®æ ‡æ³¨æ€»ç»“ - {video_name}")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"**æŠ½å¸§ç‡**: {fps} FPS")
    lines.append(f"**RAGå¢å¼º**: {'âœ… å¯ç”¨' if use_rag else 'âŒ æœªå¯ç”¨'}")
    lines.append("")
    
    # æ¦‚è§ˆç»Ÿè®¡
    lines.append("## ğŸ“ˆ æ¦‚è§ˆç»Ÿè®¡")
    lines.append("")
    lines.append(f"| æŒ‡æ ‡ | æ•°å€¼ |")
    lines.append(f"|------|------|")
    lines.append(f"| æ€»å¸§æ•° | {stats['total_frames']} |")
    lines.append(f"| æœ‰æ ‡æ³¨çš„å¸§ | {stats['annotated_frames']} |")
    lines.append(f"| ç©ºå¸§ï¼ˆæ— æ£€æµ‹ï¼‰ | {stats['total_frames'] - stats['annotated_frames']} |")
    lines.append(f"| æ€»æ£€æµ‹å¯¹è±¡ | {stats['total_objects']} |")
    if stats['annotated_frames'] > 0:
        lines.append(f"| å¹³å‡æ¯å¸§å¯¹è±¡æ•° | {stats['total_objects'] / stats['annotated_frames']:.2f} |")
    lines.append("")
    
    # ç±»åˆ«åˆ†å¸ƒ
    lines.append("## ğŸ·ï¸ ä¸»ç±»åˆ«åˆ†å¸ƒ")
    lines.append("")
    lines.append(f"| ç±»åˆ« | æ•°é‡ | å æ¯” |")
    lines.append(f"|------|------|------|")
    total = stats['total_objects'] or 1
    for cat, count in sorted(stats['categories'].items(), key=lambda x: -x[1]):
        percentage = count / total * 100
        lines.append(f"| {cat} | {count} | {percentage:.1f}% |")
    lines.append("")
    
    # ç»†åˆ†ç±»åˆ« Top 20
    if stats['subcategories']:
        lines.append("## ğŸ” ç»†åˆ†ç±»åˆ« Top 20")
        lines.append("")
        lines.append(f"| æ ‡ç­¾ | æ•°é‡ |")
        lines.append(f"|------|------|")
        for label, count in sorted(stats['subcategories'].items(), key=lambda x: -x[1])[:20]:
            # æˆªæ–­è¿‡é•¿çš„æ ‡ç­¾
            display_label = label[:50] + "..." if len(label) > 50 else label
            lines.append(f"| {display_label} | {count} |")
        lines.append("")
    
    # åœºæ™¯äº‹ä»¶
    if any(stats['scene_events'].values()):
        lines.append("## ğŸ¬ åœºæ™¯äº‹ä»¶æ£€æµ‹")
        lines.append("")
        
        event_names = {
            "braking": "ğŸ›‘ åˆ¹è½¦åœºæ™¯",
            "turn_left": "â¬…ï¸ å·¦è½¬åœºæ™¯", 
            "turn_right": "â¡ï¸ å³è½¬åœºæ™¯",
            "hazard_lights": "âš ï¸ åŒé—ª/å±é™©è­¦å‘Š",
            "pedestrian_crossing": "ğŸš¶ è¡Œäººè¿‡é©¬è·¯"
        }
        
        for event_key, event_name in event_names.items():
            frames = stats['scene_events'].get(event_key, [])
            if frames:
                lines.append(f"### {event_name}")
                lines.append(f"- **æ£€æµ‹å¸§æ•°**: {len(frames)}")
                # æ˜¾ç¤ºå‰10å¸§
                sample_frames = frames[:10]
                lines.append(f"- **ç¤ºä¾‹å¸§**: {', '.join(sample_frames)}")
                if len(frames) > 10:
                    lines.append(f"- *(å…± {len(frames)} å¸§)*")
                lines.append("")
    
    # å¸§çº§åˆ«è¯¦æƒ…ï¼ˆé‡‡æ ·æ˜¾ç¤ºï¼‰
    lines.append("## ğŸ“‹ å¸§çº§åˆ«è¯¦æƒ…ï¼ˆé‡‡æ ·ï¼‰")
    lines.append("")
    lines.append("ä»…æ˜¾ç¤ºæœ‰æ£€æµ‹å¯¹è±¡çš„å¸§ï¼ˆæœ€å¤šå‰50å¸§ï¼‰ï¼š")
    lines.append("")
    lines.append(f"| å¸§å | å¯¹è±¡æ•° | è¡Œäºº | è½¦è¾† | äº¤é€šæ ‡å¿— | æ–½å·¥ |")
    lines.append(f"|------|--------|------|------|----------|------|")
    
    shown = 0
    for frame_info in stats['frame_details']:
        if frame_info['objects'] > 0 and shown < 50:
            cats = frame_info['categories']
            lines.append(f"| {frame_info['frame']} | {frame_info['objects']} | "
                        f"{cats.get('pedestrian', 0)} | {cats.get('vehicle', 0)} | "
                        f"{cats.get('traffic_sign', 0)} | {cats.get('construction', 0)} |")
            shown += 1
    
    if shown == 0:
        lines.append("| (æ— æ£€æµ‹å¯¹è±¡) | - | - | - | - | - |")
    
    lines.append("")
    lines.append("---")
    lines.append(f"*æ­¤æŠ¥å‘Šç”± video_to_dataset.py è‡ªåŠ¨ç”Ÿæˆ*")
    
    return "\n".join(lines)


def create_dataset(video_name: str, frames_dir: Path, annotations_dir: Path, vis_dir: Path, 
                   fps: int = 3, use_rag: bool = False) -> Path:
    """åˆ›å»º Dataset æ–‡ä»¶å¤¹"""
    import shutil
    
    print(f"\nğŸ“¦ Step 4: åˆ›å»º Dataset")
    
    # è¾“å‡ºåˆ° dataset_output ç›®å½•ä¸‹
    output_base = Path("dataset_output")
    output_base.mkdir(parents=True, exist_ok=True)
    dataset_dir = output_base / f"{video_name}_dataset"
    
    # åˆ›å»ºç›®å½•ç»“æ„
    (dataset_dir / "video").mkdir(parents=True, exist_ok=True)
    (dataset_dir / "frames").mkdir(parents=True, exist_ok=True)
    (dataset_dir / "annotations").mkdir(parents=True, exist_ok=True)
    (dataset_dir / "visualized").mkdir(parents=True, exist_ok=True)
    
    # å¤åˆ¶è§†é¢‘
    video_src = VIDEO_DIR / f"{video_name}.mp4"
    if video_src.exists():
        shutil.copy(video_src, dataset_dir / "video" / f"{video_name}.mp4")
        print(f"   âœ… å¤åˆ¶è§†é¢‘")
    
    # å¤åˆ¶å¸§
    frame_count = len(list(frames_dir.glob('*.jpg')))
    for frame in frames_dir.glob("*.jpg"):
        shutil.copy(frame, dataset_dir / "frames" / frame.name)
    print(f"   âœ… å¤åˆ¶ {frame_count} å¸§")
    
    # å¤åˆ¶æ ‡æ³¨
    for ann in annotations_dir.glob("*.json"):
        shutil.copy(ann, dataset_dir / "annotations" / ann.name)
    print(f"   âœ… å¤åˆ¶ {len(list(annotations_dir.glob('*.json')))} æ ‡æ³¨")
    
    # å¤åˆ¶å¯è§†åŒ–
    if vis_dir and vis_dir.exists():
        for vis in vis_dir.glob("*.jpg"):
            shutil.copy(vis, dataset_dir / "visualized" / vis.name)
        print(f"   âœ… å¤åˆ¶ {len(list(vis_dir.glob('*.jpg')))} å¯è§†åŒ–")
    
    # ç”Ÿæˆæ€»ç»“æ–‡æ¡£
    print(f"   ğŸ“ ç”Ÿæˆæ ‡æ³¨æ€»ç»“æ–‡æ¡£...")
    stats = generate_summary(annotations_dir, video_name, frame_count, fps)
    summary_md = create_summary_markdown(stats, video_name, fps, use_rag)
    
    summary_path = dataset_dir / "SUMMARY.md"
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(summary_md)
    print(f"   âœ… ç”Ÿæˆ SUMMARY.md")
    
    # åŒæ—¶ä¿å­˜ JSON æ ¼å¼çš„ç»Ÿè®¡æ•°æ®
    stats_json = {
        "video_name": video_name,
        "total_frames": stats["total_frames"],
        "annotated_frames": stats["annotated_frames"],
        "total_objects": stats["total_objects"],
        "categories": dict(stats["categories"]),
        "subcategories": dict(stats["subcategories"]),
        "scene_events": {k: len(v) for k, v in stats["scene_events"].items()},
        "fps": fps,
        "use_rag": use_rag
    }
    stats_path = dataset_dir / "stats.json"
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(stats_json, f, ensure_ascii=False, indent=2)
    print(f"   âœ… ç”Ÿæˆ stats.json")
    
    # ç”Ÿæˆå‹ç¼©åŒ…ï¼ˆæ”¾åœ¨ dataset_output ç›®å½•ä¸‹ï¼‰
    print(f"   ğŸ“¦ åˆ›å»ºå‹ç¼©åŒ…...")
    zip_path = output_base / f"{video_name}_dataset.zip"
    shutil.make_archive(str(dataset_dir), 'zip', str(output_base), f"{video_name}_dataset")
    zip_size = zip_path.stat().st_size / (1024 * 1024)
    print(f"   âœ… {zip_path} ({zip_size:.1f} MB)")
    
    return dataset_dir


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="è§†é¢‘åˆ°æ•°æ®é›†ä¸€é”®æµæ°´çº¿")
    parser.add_argument("--video", type=str, required=True, help="è§†é¢‘åç§° (å¦‚ D3, D4)")
    parser.add_argument("--fps", type=int, default=3, help="æŠ½å¸§ç‡ (é»˜è®¤ 3)")
    parser.add_argument("--workers", type=int, default=20, help="å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤ 20)")
    parser.add_argument("--rag", action="store_true", help="å¯ç”¨ RAG ç»†ç²’åº¦åˆ†ç±»")
    parser.add_argument("--skip-extract", action="store_true", help="è·³è¿‡æŠ½å¸§æ­¥éª¤")
    parser.add_argument("--skip-visualize", action="store_true", help="è·³è¿‡å¯è§†åŒ–æ­¥éª¤")
    args = parser.parse_args()
    
    video_name = args.video
    
    print("=" * 70)
    print(f"ğŸš€ è§†é¢‘åˆ°æ•°æ®é›†æµæ°´çº¿ - {video_name}")
    print(f"   FPS: {args.fps} | Workers: {args.workers} | RAG: {args.rag}")
    print("=" * 70)
    
    start_time = time.time()
    
    # Step 1: æŠ½å¸§
    if args.skip_extract:
        frames_dir = TEMP_FRAMES_DIR / video_name
        print(f"\nâ­ï¸ è·³è¿‡æŠ½å¸§ï¼Œä½¿ç”¨: {frames_dir}")
    else:
        frames_dir, frame_count = extract_frames(video_name, args.fps)
        if not frames_dir:
            return
    
    # Step 2: æ ‡æ³¨
    annotations_dir = run_labeling(frames_dir, video_name, args.workers, args.rag)
    if not annotations_dir:
        return
    
    # Step 3: å¯è§†åŒ–
    if args.skip_visualize:
        vis_dir = None
        print(f"\nâ­ï¸ è·³è¿‡å¯è§†åŒ–")
    else:
        vis_dir = generate_visualizations(frames_dir, annotations_dir, video_name)
    
    # Step 4: æ‰“åŒ…
    dataset_dir = create_dataset(video_name, frames_dir, annotations_dir, vis_dir, 
                                  fps=args.fps, use_rag=args.rag)
    
    # å®Œæˆ
    total_time = time.time() - start_time
    print("\n" + "=" * 70)
    print(f"ğŸ‰ å®Œæˆï¼æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“ Dataset: {dataset_dir}/")
    print(f"ğŸ“Š æ€»ç»“æ–‡æ¡£: {dataset_dir}/SUMMARY.md")
    print(f"ğŸ“¦ å‹ç¼©åŒ…: {dataset_dir}.zip")
    print("=" * 70)


if __name__ == "__main__":
    main()
